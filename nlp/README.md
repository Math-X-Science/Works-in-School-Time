# 在做什么？

我在尝试利用`deeplx`翻译英文小说，形成翻译对，然后用这些翻译对微调和从 0 训练 Transformer 模型。<br>

感谢这些仓库:<br>

- [https://github.com/CjangCjengh/YakuYaku](https://github.com/CjangCjengh/YakuYaku)
- [datawhalechina/learn-nlp-with-transformers](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.6-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.ipynb)

## 关于数据集：

- 考虑到我们的数据集一半是翻译模型自动生成的。而模型碰到一些特殊词汇或者有时候算力紧张就会吐原文（英文），所以最重要的反而是把 csv 里面中文栏内含有英文的部分条目都清洗掉。因为这代表着这个段落没有被翻译完全，应该被清洗。<br>
- 关于 csv 存储的分隔问题，我最初制作数据集的时候用的是`,`，英文的，但是实际上这么做存在隐患，存在什么隐患，那就是必须保证我的中文栏目里面一个英文逗号都没有（它也确实是这样的），否则会导致读取错乱。对于这种没有处理特殊符号的数据集，BEST PRACTICE 还是用`|`。

因为数据集合并后有点大，存在 github 上面影响 clone 的时间，所以这里我打算把它抽离到网盘里。<br>

它分成这么几个部分：<br>

- origin_dataset/: 它包含原始的英文 txt 和按`\n`切分和翻译后的翻译对`csv`,用`,`分割。里面存在一些中英文混合的条目，需要清洗。<br>
- cleaned_dataset/: 它包含清洗后的翻译对`csv`，依然是用`,`分割,(我意识到地太晚了)，但因为中文里面不存在英文,所以不需要额外处理可以直接读取。<br>
- eval.csv: 我随便生成的几个翻译对，用于测试模型的效果。你可以自定义。<br>
- small.csv: 我随便挑出来的几个翻译对，用来跑通模型用的。我建议你也先验证一下模型是否能跑通最终再在大数据集上训练，因为猥琐的模型总是在最后一步才会报错。<br>
- noval.csv: 我把清洗后的数据集简单地合并和叠加到一起的结果。<br>

你可以自己对原始数据集做自己的需要的处理，比如你还可以去除掉所有短句。<br>

以及中英文混合的例子让我想起来以前训练`BERT-VITS2`的时候，在文本样例中，混合样例比较少，但是对于说话的场景，那中英文混合的案例就太多了：<br>

- 比如说我们这个 tensor 啊。
- pytorch 真的是太好用了。
- coding 能力要怎么样才能快速提升或者稳步提升？

这样子的样例太多了。<br>

所以我觉得中英文混在一起做`tokenize`应该是没有问题的，甚至可能会在上面的问题上表现的不错。后面 BERT-VITS2 也确实出了一个支持中英文混合的模型，但我当时已经没有再跟进了，没看细节。<br>

但那种混合的情况数据集太难找了。把各种音素的混合训练到，哪怕只是单语言就够喝一壶了，混合的话，数据集的均衡性非常难保证。<br>

数据集网盘地址:<br>

```txt
english-novel-complete-works
链接: https://pan.baidu.com/s/1eh4D0at0ONT7Ct89V0Mw8g?pwd=1tt3 提取码: 1tt3
```

## 我突然想起来。

我得先去把计算机视觉的那个代码大致写一下。那个截至更早一些。<br>

先溜了。<br>
