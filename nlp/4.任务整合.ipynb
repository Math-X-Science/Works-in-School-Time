{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# å¾®è°ƒtransformeræ¨¡å‹è§£å†³ç¿»è¯‘ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "rJvQjiUqPjhM"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"./opus-mt-zh-en\" \n",
    "# é€‰æ‹©ä¸€ä¸ªæ¨¡å‹checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "åªè¦é¢„è®­ç»ƒçš„transformeræ¨¡å‹åŒ…å«seq2seqç»“æ„çš„headå±‚ï¼Œé‚£ä¹ˆæœ¬notebookç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹[æ¨¡å‹é¢æ¿](https://huggingface.co/models)ï¼Œè§£å†³ä»»ä½•ç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## åŠ è½½æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 6\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# è¯»å–CSVæ–‡ä»¶\n",
    "df = pd.read_csv('small.csv', names=['en', 'zh'],sep=\"|\")  # å‡è®¾æ–‡ä»¶æ²¡æœ‰å¤´éƒ¨ï¼Œç¬¬ä¸€åˆ—ä¸ºè‹±æ–‡ï¼Œç¬¬äºŒåˆ—ä¸ºç½—é©¬å°¼äºšè¯­\n",
    "eval_df = pd.read_csv(\"eval.csv\",names=['en','zh'],sep=\"|\")\n",
    "# åˆ›å»ºä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ç¿»è¯‘å¯¹\n",
    "translations = [{'translation': {'en': row['en'], 'zh': row['zh']}} for _, row in df.iterrows()]\n",
    "validation_translations = [{'translation': {'en': row['en'], 'zh': row['zh']}} for _, row in eval_df.iterrows()]\n",
    "# åˆ›å»ºä¸€ä¸ªDataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(translations))\n",
    "validation_dataset = Dataset.from_pandas(pd.DataFrame(validation_translations))\n",
    "\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': dataset,\n",
    "    'val': validation_dataset\n",
    "})\n",
    "\n",
    "# æŸ¥çœ‹è½¬æ¢åçš„æ•°æ®é›†\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "è¿™ä¸ªdatasetså¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)æ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚\n",
    "\n",
    "éå¸¸æ–¹ä¾¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6HrpprwIrIz",
    "outputId": "69f3873e-2d1f-4614-e43e-9e654277245c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'English', 'zh': 'ä¸­æ–‡'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## éšæœºå¯è§†åŒ–æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "SZy5tRB_IrI7",
    "outputId": "93e16172-d927-457d-fcab-04dcb4d2ef29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Liza put on the air of a conquering hero, and sauntered on, enchanted at the uproar. She stuck out her elbows and jerked her head on one side, and said to herself as she passed through the bellowing crowd:', 'zh': 'è‰èæ‘†å‡ºä¸€å‰¯å¾æœè‹±é›„çš„å§¿æ€ï¼Œåœ¨å–§é—¹å£°ä¸­é™¶é†‰åœ°å¤§æ­¥å‘å‰èµ°å»ã€‚å¥¹ä¼¸å‡ºèƒ³è†Šè‚˜ï¼ŒæŠŠå¤´æ‰­å‘ä¸€è¾¹ï¼Œåœ¨ç©¿è¿‡å–§é—¹çš„äººç¾¤æ—¶è‡ªè¨€è‡ªè¯­é“ï¼š'è¿™æ‰å«æœé…±ï¼''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'â€˜Come on, Florrie, you and me ainâ€™t shy; weâ€™ll begin, and bust it!â€™', 'zh': 'æ¥å§ï¼Œå¼—æ´›è‰ï¼Œä½ å’Œæˆ‘éƒ½ä¸å®³ç¾ï¼›æˆ‘ä»¬å¼€å§‹å§ï¼Œè·³ä¸ªç—›å¿«ï¼\"ä¸¤ä¸ªå¥³å­©äº’ç›¸æ‹‰ç€å¯¹æ–¹çš„æ‰‹ã€‚'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'English', 'zh': 'ä¸­æ–‡'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': 'When she came to the group round the barrel-organ, one of the girls cried out to her:', 'zh': 'å½“å¥¹èµ°åˆ°å›´ç€æœ¨æ¡¶é£ç´çš„äººç¾¤ä¸­æ—¶ï¼Œä¸€ä¸ªå¥³å­©å¯¹å¥¹å–Šé“ï¼š'è¿™æ˜¯ä½ çš„æ–°è¡£æœå—ï¼Ÿ'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'It was a young girl of about eighteen, with dark eyes, and an enormous fringe, puffed-out and curled and frizzed, covering her whole forehead from side to side, and coming down to meet her eyebrows. She was dressed in brilliant violet, with great lappets of velvet, and she had on her head an enormous black hat covered with feathers.', 'zh': 'é‚£æ˜¯ä¸€ä¸ªå¤§çº¦åå…«å²çš„å¹´è½»å¥³å­©ï¼Œå¥¹æœ‰ä¸€åŒä¹Œé»‘çš„çœ¼ç›ï¼Œä¸€å¤´è“¬æ¾ã€å·æ›²ã€æ¯›èŒ¸èŒ¸çš„å·¨å¤§æµè‹ä»ä¸¤ä¾§è¦†ç›–äº†æ•´ä¸ªé¢å¤´ï¼Œä¸€ç›´å‚åˆ°çœ‰æ¯›ä¸Šã€‚å¥¹ç©¿ç€è‰³ä¸½çš„ç´«ç½—å…°è‰²è¡£æœï¼Œä¸Šé¢é•¶åµŒç€å·¨å¤§çš„å¤©é¹…ç»’èŠ±è¾¹ï¼Œå¤´ä¸Šæˆ´ç€ä¸€é¡¶å·¨å¤§çš„é»‘è‰²å¸½å­ï¼Œä¸Šé¢æ’æ»¡äº†ç¾½æ¯›ã€‚'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "metricæ˜¯[`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric)ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼ŒæŸ¥çœ‹metricå’Œä½¿ç”¨çš„ä¾‹å­:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# åŠ è½½ä¸€ä¸ªæŒ‡æ ‡ï¼Œæ¯”å¦‚ BLEU\n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5o4rUteaIrI_",
    "outputId": "4814f907-6225-4af0-ee63-376699dc79ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "## è¯„åˆ†è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XN1Rq0aIrJC",
    "outputId": "d130ad50-c6ca-42bc-8b14-31021feb620d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 0, 0],\n",
       " 'totals': [4, 2, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚é¢„å¤„ç†çš„å·¥å…·å«Tokenizerã€‚Tokenizeré¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "ä¸ºäº†è¾¾åˆ°æ•°æ®é¢„å¤„ç†çš„ç›®çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨AutoTokenizer.from_pretrainedæ–¹æ³•å®ä¾‹åŒ–æˆ‘ä»¬çš„tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š\n",
    "\n",
    "- æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚\n",
    "- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizerçš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¹Ÿä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚\n",
    "\n",
    "\n",
    "è¿™ä¸ªè¢«ä¸‹è½½çš„tokens vocabularyä¼šè¢«ç¼“å­˜èµ·æ¥ï¼Œä»è€Œå†æ¬¡ä½¿ç”¨çš„æ—¶å€™ä¸ä¼šé‡æ–°ä¸‹è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xnne/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# éœ€è¦å®‰è£…`sentencepiece`ï¼š pip install sentencepiece\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLRyc5J9PjhS"
   },
   "source": [
    "ä»¥æˆ‘ä»¬ä½¿ç”¨çš„mBARTæ¨¡å‹ä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦æ­£ç¡®è®¾ç½®sourceè¯­è¨€å’Œtargetè¯­è¨€ã€‚å¦‚æœæ‚¨è¦ç¿»è¯‘çš„æ˜¯å…¶ä»–åŒè¯­è¯­æ–™ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](https://huggingface.co/facebook/mbart-large-cc25)ã€‚æˆ‘ä»¬å¯ä»¥æ£€æŸ¥sourceå’Œtargetè¯­è¨€çš„è®¾ç½®ï¼š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word -> index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kmXG36baPjhS"
   },
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"zh-CN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "tokenizeræ—¢å¯ä»¥å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå¯ä»¥å¯¹ä¸€å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œtokenizeré¢„å¤„ç†åå¾—åˆ°çš„æ•°æ®æ»¡è¶³é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5hBlsrHIrJL",
    "outputId": "072ee20c-db1d-4ba1-a98a-119405ea9552"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1496, 26607, 2, 56, 7, 5112, 95, 4509, 8233, 48, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "ä¸Šé¢çœ‹åˆ°çš„token IDsä¹Ÿå°±æ˜¯input_idsä¸€èˆ¬æ¥è¯´éšç€é¢„è®­ç»ƒæ¨¡å‹åå­—çš„ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚åŸå› æ˜¯ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™è®¾å®šäº†ä¸åŒçš„è§„åˆ™ã€‚ä½†åªè¦tokenizerå’Œmodelçš„åå­—ä¸€è‡´ï¼Œé‚£ä¹ˆtokenizeré¢„å¤„ç†çš„è¾“å…¥æ ¼å¼å°±ä¼šæ»¡è¶³modeléœ€æ±‚çš„ã€‚å…³äºé¢„å¤„ç†æ›´å¤šå†…å®¹å‚è€ƒ[è¿™ä¸ªæ•™ç¨‹](https://huggingface.co/transformers/preprocessing.html)\n",
    "\n",
    "é™¤äº†å¯ä»¥tokenizeä¸€å¥è¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥tokenizeä¸€ä¸ªlistçš„å¥å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkLffVlKPjhT",
    "outputId": "f144d050-fc84-4a1a-9fc2-25281b681441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1496, 26607, 2, 56, 7, 5112, 95, 4509, 8233, 48, 0], [206, 30, 7, 16689, 19114, 95, 4509, 8233, 5, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uVqYJrePjhT"
   },
   "source": [
    "æ³¨æ„ï¼šä¸ºäº†ç»™æ¨¡å‹å‡†å¤‡å¥½ç¿»è¯‘çš„targetsï¼Œæˆ‘ä»¬ä½¿ç”¨`as_target_tokenizer`æ¥æ§åˆ¶targetsæ‰€å¯¹åº”çš„ç‰¹æ®Štokenï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgCW0X0FPjhT",
    "outputId": "352c44ab-f025-4cf6-98d1-786f6f07111a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3833, 2, 56, 139, 4839, 48, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "tokens: ['â–H', 'ello', ',', 'â–this', 'â–', 'one', 'â–s', 'ent', 'ence', '!', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xnne/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer(\"Hello, this one sentence!\"))\n",
    "    model_input = tokenizer(\"Hello, this one sentence!\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n",
    "    # æ‰“å°çœ‹ä¸€ä¸‹special toke\n",
    "    print('tokens: {}'.format(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯T5é¢„è®­ç»ƒæ¨¡å‹çš„checkpointsï¼Œéœ€è¦å¯¹ç‰¹æ®Šçš„å‰ç¼€è¿›è¡Œæ£€æŸ¥ã€‚T5ä½¿ç”¨ç‰¹æ®Šçš„å‰ç¼€æ¥å‘Šè¯‰æ¨¡å‹å…·ä½“è¦åšçš„ä»»åŠ¡ï¼Œå…·ä½“å‰ç¼€ä¾‹å­å¦‚ä¸‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "xS1JJSdmPjhU"
   },
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CezpZ8gFPjhU"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰å†…å®¹æ”¾åœ¨ä¸€èµ·ç»„æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°äº†ã€‚æˆ‘ä»¬å¯¹æ ·æœ¬è¿›è¡Œé¢„å¤„ç†çš„æ—¶å€™ï¼Œæˆ‘ä»¬è¿˜ä¼š`truncation=True`è¿™ä¸ªå‚æ•°æ¥ç¡®ä¿æˆ‘ä»¬è¶…é•¿çš„å¥å­è¢«æˆªæ–­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹ä¸æ¯”è¾ƒçŸ­çš„å¥å­æˆ‘ä»¬ä¼šè‡ªåŠ¨paddingã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"zh\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-b70jh26IrJS",
    "outputId": "89b26088-d2d2-4312-81d8-b0f5e62dd6a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[471, 3920, 3707, 6436, 0], [7, 21042, 1481, 17210, 2172, 10955, 25, 7, 21042, 177, 129, 1246, 22, 7, 1813, 7, 2032, 1813, 48, 1246, 2482, 3804, 250, 7, 17933, 589, 12, 147, 3908, 1685, 7, 8378, 6807, 2515, 7, 2757, 18, 3936, 5, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7, 21116, 0], [83, 1, 2, 1, 48, 21, 1, 0]]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "æ¥ä¸‹æ¥å¯¹æ•°æ®é›†datasetsé‡Œé¢çš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå¤„ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨mapå‡½æ•°ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "DDtsaJeVIrJT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "æ›´å¥½çš„æ˜¯ï¼Œè¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ã€‚æ¸…ç†çš„æ–¹å¼æ˜¯ä½¿ç”¨`load_from_cache_file=False`å‚æ•°ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„`batched=True`è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œä»¥ä¸ºè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## å¾®è°ƒtransformeræ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "æ—¢ç„¶æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚æ—¢ç„¶æˆ‘ä»¬æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSeq2SeqLM`è¿™ä¸ªç±»ã€‚å’Œtokenizerç›¸ä¼¼ï¼Œ`from_pretrained`æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œå°±ä¸ä¼šé‡å¤ä¸‹è½½æ¨¡å‹å•¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "ç”±äºæˆ‘ä»¬å¾®è°ƒçš„ä»»åŠ¡æ˜¯æœºå™¨ç¿»è¯‘ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒçš„seq2seqæ¨¡å‹ï¼Œæ‰€ä»¥ä¸ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†æœºå™¨ç¿»è¯‘çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Seq2SeqTrainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦3ä¸ªè¦ç´ ï¼Œå…¶ä¸­æœ€é‡è¦çš„æ˜¯è®­ç»ƒçš„è®¾å®š/å‚æ•°[`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xnne/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 6\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-translation\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "ä¸Šé¢evaluation_strategy = \"epoch\"å‚æ•°å‘Šè¯‰è®­ç»ƒä»£ç ï¼šæˆ‘ä»¬æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ã€‚\n",
    "\n",
    "ä¸Šé¢batch_sizeåœ¨è¿™ä¸ªnotebookä¹‹å‰å®šä¹‰å¥½äº†ã€‚\n",
    "\n",
    "ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†æ¯”è¾ƒå¤§ï¼ŒåŒæ—¶`Seq2SeqTrainer`ä¼šä¸æ–­ä¿å­˜æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒè‡³å¤šä¿å­˜`save_total_limit=3`ä¸ªæ¨¡å‹ã€‚\n",
    "\n",
    "æœ€åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†æˆ‘ä»¬å¤„ç†å¥½çš„è¾“å…¥å–‚ç»™æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "ZMdgZaOoPjhX"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "è®¾ç½®å¥½`Seq2SeqTrainer`è¿˜å‰©æœ€åä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯æˆ‘ä»¬éœ€è¦å®šä¹‰å¥½è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨`metric`æ¥å®Œæˆè¯„ä¼°ã€‚å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåšä¸€äº›æ•°æ®åå¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds if pred.strip()]\n",
    "    labels = [[label.strip() for label in label_list if label.strip()] for label_list in labels]\n",
    "    labels = [label_list for label_list in labels if label_list]  # è¿‡æ»¤æ‰ç©ºçš„å‚è€ƒåˆ—è¡¨\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # ç¡®ä¿æ¯ä¸ªé¢„æµ‹éƒ½æœ‰ä¸€ä¸ªå‚è€ƒç¿»è¯‘\n",
    "    references_per_prediction = max(len(refs) for refs in decoded_labels) if decoded_labels else 0\n",
    "    \n",
    "    valid_indices = [i for i, (pred, label) in enumerate(zip(decoded_preds, decoded_labels)) if pred.strip() and any(l.strip() for l in label)]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(\"Warning: No valid predictions or references. BLEU score computation skipped.\")\n",
    "        return {\"bleu\": 0.0, \"gen_len\": 0.0}\n",
    "    \n",
    "    valid_preds = [decoded_preds[i] for i in valid_indices]\n",
    "    valid_labels = [refs for i, refs in enumerate(decoded_labels) if i in valid_indices]\n",
    "    \n",
    "    # ç¡®ä¿æ¯ä¸ªé¢„æµ‹æœ‰ç›¸åŒæ•°é‡çš„å‚è€ƒ\n",
    "    valid_labels = [refs if len(refs) == references_per_prediction else [refs[0]] * references_per_prediction for refs in valid_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=valid_preds, references=valid_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "æœ€åå°†æ‰€æœ‰çš„å‚æ•°/æ•°æ®/æ¨¡å‹ä¼ ç»™`Seq2SeqTrainer`å³å¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207329/21586541.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•è¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.57it/s]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                    \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "                                             \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  1.57it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                     \n",
      "                                             \n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:08<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.0219037532806396, 'eval_bleu': 1.0683, 'eval_gen_len': 19.5, 'eval_runtime': 3.9333, 'eval_samples_per_second': 1.525, 'eval_steps_per_second': 0.254, 'epoch': 1.0}\n",
      "{'train_runtime': 8.7425, 'train_samples_per_second': 2.86, 'train_steps_per_second': 0.572, 'train_loss': 1.2025309562683106, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=1.2025309562683106, metrics={'train_runtime': 8.7425, 'train_samples_per_second': 2.86, 'train_steps_per_second': 0.572, 'total_flos': 707628367872.0, 'train_loss': 1.2025309562683106, 'epoch': 1.0})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXOyGJtqPjhZ"
   },
   "source": [
    "æœ€ååˆ«å¿˜äº†ï¼ŒæŸ¥çœ‹å¦‚ä½•ä¸Šä¼ æ¨¡å‹ ï¼Œä¸Šä¼ æ¨¡å‹åˆ°](https://huggingface.co/transformers/model_sharing.html) åˆ°[ğŸ¤— Model Hub](https://huggingface.co/models)ã€‚éšåæ‚¨å°±å¯ä»¥åƒè¿™ä¸ªnotebookä¸€å¼€å§‹ä¸€æ ·ï¼Œç›´æ¥ç”¨æ¨¡å‹åå­—å°±èƒ½ä½¿ç”¨æ‚¨çš„æ¨¡å‹å•¦ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jeq1Cq2yPjhZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
